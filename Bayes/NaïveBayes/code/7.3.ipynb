{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7.3\n",
    "# Implemetation of NaÃ¯ve Bayes Classifier with Laplacian Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Conditional Probability Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The prior probability of a class can be estimated as\n",
    "$$ \\hat{P} (c) = \\frac{|D_{c}| + 1}{|D|+ N} $$\n",
    "\n",
    "- The conditional probability $ \\hat{P}(x_{i}|c) $ can be estimated as\n",
    "$$ \\hat{P} (x_{i}|c) = \\frac{|D_{c,x_{i}}|+1}{|D_{c}| + N_{i}} $$\n",
    "\n",
    "or the PDF of the supposed distribution for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prior_prob(df_target, c):\n",
    "    \"\"\"\n",
    "    This function returns the prior probaility of a class, c, after Laplacian Correction\n",
    "    It has two inputs:\n",
    "    1. df_target: the train df column of target\n",
    "    2. c: value of class, c\n",
    "    \"\"\"\n",
    "    return (sum([1 for e in df_target if e == c]) + 1)/(len(df_target) + len(set(df_target)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def condition_prob(df_train, target_name, c, x_name, x_value, dtype):\n",
    "    \"\"\"\n",
    "    This function returns the conditional probability of a class, c, \n",
    "    after Laplacian Correction.\n",
    "    It has six inputs:\n",
    "    1. df_train: the train df\n",
    "    2. target name: the column name of the target\n",
    "    3. c: value of class, c\n",
    "    4. x_name: the column name of variable, x\n",
    "    5. x_value: the i-th value of x\n",
    "    6. dtype: the dtype of x\n",
    "    \"\"\"\n",
    "    \n",
    "    d_c = df_train[df_train[target_name] == c] # the sub_df_1 of class c\n",
    "    \n",
    "    # for continuous variables\n",
    "    if np.issubdtype(dtype, np.number):\n",
    "        u = np.mean(d_c[x_name])\n",
    "        sigma = np.std(d_c[x_name])\n",
    "        return scipy.stats.norm(u, sigma) # here suppose normal distribution\n",
    "    \n",
    "    # for category variables\n",
    "    else:\n",
    "        # the sub_df_2 of x == x_value of d_c\n",
    "        d_c_x = d_c[d_c[x_name]==x_value] \n",
    "        \n",
    "        # the number of unqiue class of sub_df_3 of x == x_value of train_df\n",
    "        n_i = len(set(df_train[target_name][df_train[x_name]==x_value])) \n",
    "        return (len(d_c_x)+1)/(len(d_c)+n_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_train(df_train, target_name, types):\n",
    "    \"\"\"\n",
    "    This function return the dictionary of all the probabilities\n",
    "    It has three inputs:\n",
    "    1. df_train: train df, including target\n",
    "    2. target_name: string, the column name of target\n",
    "    3. types: the dtypes of the df\n",
    "    \"\"\"\n",
    "    # build the dictionary\n",
    "    train_dict = {'prior':{}, 'conditional':{}}\n",
    "    \n",
    "    # select the feature names, except target name\n",
    "    cols = list(df_train.columns)\n",
    "    cols.remove(target_name)\n",
    "    \n",
    "    # compute the prior probability\n",
    "    for c in set(df_train[target_name]):\n",
    "        train_dict['prior'][c] = prior_prob(df_train[target_name], c)\n",
    "        train_dict['conditional'][c] = {}\n",
    "    \n",
    "    # compute the conditional probability\n",
    "    for col in cols: # select x\n",
    "        for c in train_dict['conditional'].keys(): # select c\n",
    "            train_dict['conditional'][c][col] = {}\n",
    "            \n",
    "            # for continuous variables\n",
    "            if np.issubdtype(types[cols.index(col)], np.number): \n",
    "                # computing the distribution\n",
    "                train_dict['conditional'][c][col] = condition_prob(df_train, \n",
    "                                                                   target_name,\n",
    "                                                                   c, col, x_value,\n",
    "                                                                   types[cols.index(col)])\n",
    "            else: # for category variables\n",
    "                for x_value in set(df_train[col]):\n",
    "                    # computing the condititonal probability of c_xi\n",
    "                    train_dict['conditional'][c][col][x_value] = condition_prob(df_train, \n",
    "                                                                                target_name,\n",
    "                                                                                c, col, x_value,\n",
    "                                                                                types[cols.index(col)])\n",
    "            \n",
    "    return train_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_predict(df_validation, classes, train_dict):\n",
    "    \"\"\"\n",
    "    This function returns prediction values of df_validation based on train_dict\n",
    "    It has three inputs:\n",
    "    1. df_validation: validation datafram\n",
    "    2. classes: the list/set of all possible classes\n",
    "    3. train_dict: dict, the trained dictionary of bayes classification\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = df_validation.columns\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(len(df_validation)): # iterate through the df\n",
    "        p = 0\n",
    "        variables = df_validation.iloc[i] # features variables\n",
    "        for c in classes:\n",
    "            prob = train_dict['prior'][c] # get the prior probability\n",
    "            for col in cols:\n",
    "                try: # for category variables\n",
    "                    \n",
    "                    # compute p(c) * p(x_i|c)\n",
    "                    prob *= train_dict['conditional'][c][col][variables[col]]\n",
    "                except TypeError: # for continuous variables\n",
    "                    \n",
    "                    # get distribution\n",
    "                    norm = train_dict['conditional'][c][col] \n",
    "                    \n",
    "                    # compute p(c) * p(x_i|c)\n",
    "                    prob *= train_dict['conditional'][c][col].pdf(variables[col]) \n",
    "            \n",
    "            # storing the class with the largest probability\n",
    "            if prob > p:\n",
    "                p = prob\n",
    "                prediction = c\n",
    "                \n",
    "        # storing the predictions\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Data and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/data.txt').drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test set splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_tr = data.iloc[[0,1,2,5,6,9,13,14,15,16]]\n",
    "data_test = data.drop(data_tr.index)\n",
    "data_test_x, data_test_y = data_test.drop(['quality'],axis=1), data_test.quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conditional': {'bad': {'color': {'dark-green': 0.42857142857142855,\n",
       "    'pitch-dark': 0.2857142857142857,\n",
       "    'white': 0.5},\n",
       "   'density': <scipy.stats._distn_infrastructure.rv_frozen at 0x10c4b2b00>,\n",
       "   'root': {'roll-up': 0.42857142857142855,\n",
       "    'slighly-curled': 0.42857142857142855,\n",
       "    'stiff': 0.3333333333333333},\n",
       "   'sound': {'crisp': 0.3333333333333333,\n",
       "    'dead': 0.42857142857142855,\n",
       "    'dull': 0.42857142857142855},\n",
       "   'stripes': {'blurred': 0.3333333333333333,\n",
       "    'clear': 0.42857142857142855,\n",
       "    'indistinct': 0.42857142857142855},\n",
       "   'sugar': <scipy.stats._distn_infrastructure.rv_frozen at 0x10c4b26a0>,\n",
       "   'touch': {'hard': 0.5714285714285714, 'soft': 0.42857142857142855},\n",
       "   'umbilical': {'hollow': 0.2857142857142857,\n",
       "    'plain': 0.5,\n",
       "    'slightly-hollow': 0.42857142857142855}},\n",
       "  'good': {'color': {'dark-green': 0.42857142857142855,\n",
       "    'pitch-dark': 0.5714285714285714,\n",
       "    'white': 0.16666666666666666},\n",
       "   'density': <scipy.stats._distn_infrastructure.rv_frozen at 0x10c4b2cc0>,\n",
       "   'root': {'roll-up': 0.5714285714285714,\n",
       "    'slighly-curled': 0.42857142857142855,\n",
       "    'stiff': 0.16666666666666666},\n",
       "   'sound': {'crisp': 0.16666666666666666,\n",
       "    'dead': 0.2857142857142857,\n",
       "    'dull': 0.7142857142857143},\n",
       "   'stripes': {'blurred': 0.16666666666666666,\n",
       "    'clear': 0.7142857142857143,\n",
       "    'indistinct': 0.2857142857142857},\n",
       "   'sugar': <scipy.stats._distn_infrastructure.rv_frozen at 0x10c4b25c0>,\n",
       "   'touch': {'hard': 0.5714285714285714, 'soft': 0.42857142857142855},\n",
       "   'umbilical': {'hollow': 0.5714285714285714,\n",
       "    'plain': 0.16666666666666666,\n",
       "    'slightly-hollow': 0.42857142857142855}}},\n",
       " 'prior': {'bad': 0.5, 'good': 0.5}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = bayes_train(data_tr, 'quality', data_tr.dtypes)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = bayes_predict(data_test_x, set(data_tr.quality), d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "sum([data_test_y.iloc[i] == pred[i] for i in range(len(pred))]) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
